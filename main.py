import os
import sys
import requests
from pathlib import Path
from datasets import Dataset, load_dataset, DatasetDict
from openai import OpenAI
from langchain.text_splitter import TokenTextSplitter
import random
import pandas
import pickle
from multiprocessing.pool import ThreadPool
import glob
import re

def strip_str(s: str) -> str:
    """
    Helper function for helping format strings returned by GPT-4.
    """
    s = s.lstrip('0123456789.- :一二三四五六七八九十、').strip().replace("\n", "").replace("\r", "").replace("\t", "")
    return s

def fix_resp(resp: str) -> str:
    """
    Helper function for fixing responses generated by GPT-4.
    """
    resp = resp.replace("<ANSWER:>", "<ANSWER>:")
    return resp

def generate_q_taide(headers, requests, chunk, num_q):
    messages_q = [
        {"role": "system", "content": f"您是一個合成問答配對生成器。給定一段關於某個主題的內容，生成 {num_q} 個示例問題，用戶可以提出這些問題，並且可以通過這段內容中的信息進行回答。例如，如果給定的內容是維基百科關於台灣的一段文字，一個示例問題可以是“台灣有多少個縣？”。"},
        {"role": "system", "content": "這些示例問題的答案要很簡短或是幾個字。您的回覆中只需包含問題即可。"},
        {"role": "user", "content": str(chunk)}
    ]
    data = {
    "model": "taide-llama2-70b",
    "messages": messages_q,
    "max_tokens": 512,
    "temperature": 0
    }
    r = requests.post(host+"/chat/completions", json=data, headers=headers)
    res = r.json()["choices"][0]["message"]
    queries = res["content"].split('\n')
    queries = [strip_str(q) for q in queries]
    return queries

def generate_a_taide(headers, requests, chunk, question):
    prompts = []
    prompt = f"""
        問題：{question}\n內文：{chunk}\n
        使用上述內容中提供的資訊回答這個問題。以下是需要注意的事項：
        - 首先提供逐步推理，解釋如何回答問題。
        - 在推理過程中，如果需要從內容中複製粘貼一些句子，請將它們放在##begin_quote##和##end_quote##中。
        這意味著##begin_quote##和##end_quote##之外的內容不是直接從內容中複製粘貼的。
        - 最後以 <ANSWER>: $answer 回答最終答案。answer 需要簡潔，不需要太長。
        您的回答必須以標籤<ANSWER>:開頭。
    """
    prompts.append({"role": "system", "content": "你是一位樂於助人的問答者，可以根據問題和相關上下文提供答案。"})
    prompts.append({"role": "user", "content": prompt})

    data = {
    "model": "taide-llama2-70b",
    "messages": prompts,
    "max_tokens": 512,
    "temperature": 0
    }
    r = requests.post(host+"/chat/completions", json=data, headers=headers)
    res = r.json()["choices"][0]["message"]
    response = res["content"]
    return response

def generate_q(client, chunk, num_q, model="gpt-4-0613"):
    messages_q = [
        {"role": "system", "content": f"您是一個合成問答配對生成器。給定一段關於某個主題的內容，生成 {num_q} 個示例問題，用戶可以提出這些問題，並且可以通過這段內容中的信息進行回答。例如，如果給定的內容是維基百科關於台灣的一段文字，一個示例問題可以是“台灣有多少個縣？”。"},
        {"role": "system", "content": "這些示例問題的答案要很簡短或是幾個字。您的回覆中只需包含問題即可。"},
        {"role": "user", "content": str(chunk)}
    ]
    response = client.chat.completions.create(
        model=model,
        messages=messages_q,
        max_tokens=512,
        temperature=0
    )

    queries = response.choices[0].message.content.split('\n')
    queries = [strip_str(q) for q in queries]
    return queries

def generate_a(client, chunk, question, model="gpt-4-0613"):
    prompts = []
    prompt = f"""
        問題：{question}\n內文：{chunk}\n
        使用上述內容中提供的資訊回答這個問題。以下是需要注意的事項：
        - 首先提供逐步推理，解釋如何回答問題。
        - 在推理過程中，如果需要從內容中複製粘貼一些句子，請將它們放在 ##begin_quote## 和 ##end_quote## 中。
        這意味著 ##begin_quote ## 和 ##end_quote## 之外的內容不是直接從內容中複製粘貼的。
        - 最後以 <ANSWER>: $answer 回答最終答案。answer 需要簡潔，不需要太長。
        您的回答必須以標籤<ANSWER>:開頭。
    """
    prompts.append({"role": "system", "content": "你是一位樂於助人的問答者，可以根據問題和相關上下文提供答案。"})
    prompts.append({"role": "user", "content": prompt})

    response = client.chat.completions.create(
        model=model,
        messages=prompts,
        n=1,
        temperature=0
    )
    response = response.choices[0].message.content
    return response

def get_chunks(text, chunk_size):
    chunker = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=5)
    chunks = chunker.split_text(text)
    return chunks

def prepare_sample(client, headers, requests, chunk, q, chunks, i, j):
    try:
        if safemode:
            if len(chunks) == 1:
                return None

        # Generate A
        if safemode:
            a = "由於RAG提供的資訊不足，模型無法正確地回答這個問題。為了能夠提供更多的幫助，請您提供更多相關的資訊，感謝您。"
        else:
            if USE_TAIDE:
                a = generate_a_taide(headers, requests, chunk, q)
            else:
                a = generate_a(client, chunk, q)
            a = fix_resp(a)
        print(f"{prefix}i#{i}@j#{j}@q#{q}@a#{len(a)}")

        # Prepare D
        docs = [chunk]
        indices = list(range(0, len(chunks)))
        if len(indices) >= 3:
            indices.remove(j)
            for k in random.sample(indices, 2):
                docs.append(chunks[k])
        elif len(indices) == 2:
            indices.remove(j)
            for k in random.sample(indices, 1):
                docs.append(chunks[k])
        oracle = random.uniform(0, 1) < ORACLE_PRIORITY
        if not oracle or safemode:
            docs[0] = chunks[random.sample(indices, 1)[0]]
        random.shuffle(docs)

        # Construct instruction
        inst_before = random.uniform(0, 1) < ORACLE_PRIORITY
        if not inst_before:
            instruction = ""
        else:
            instruction = f"{q}\n"
        for doc in docs:
            instruction += "<DOCUMENT>" + str(doc) + "</DOCUMENT>\n"
        if not inst_before:
            instruction += q

        # Add to dataset
        chat = [
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": a}
        ]
        sample = {
            'messages': chat
        }
    except Exception as e:
        print(f"{prefix}i#{i}@j#{j}@q#{q}@e#{e}")
        sample = None
    return sample


# RAFT: Adapting Language Model to Domain Specific RAG
# https://arxiv.org/abs/2403.10131
# load source data for the generation of RAFT dataset
if sys.argv[1] is not None:
    datapath = Path(sys.argv[1])
else:
    # datapath = Path("/home/bhlin6nctu/models/dataset/gov/CP/3_010_法規資料庫/*.jsonl")
    raise Exception("datapath is None")
ds = load_dataset("json", data_files=str(datapath))
print(ds)


#######################
### 50% and D* + 2D ###
#######################
# API usage documentation
# https://docs.google.com/document/d/1P1tSs5lY_TnzDtWOKLYJWFd-0iugYjWF5lY9yr5NAD8/edit#heading=h.sivcutdm86gr
host = "https://td.nchc.org.tw/api/v1"
username = os.environ.get("username")
password = os.environ.get("password")
assert username is not None, "username is None"
assert password is not None, "password is None"
r = requests.post(host+"/token", data={"username":username, "password":password})
token = r.json()["access_token"]
headers = {
"Authorization": "Bearer "+token
}

USE_TAIDE = True
safemode = False

if safemode:
    NUM_Q = 8
    NUM_CHUNK = 2
else:
    NUM_Q = 2
    NUM_CHUNK = 4
CHUNK_SIZE = 512
ORACLE_PRIORITY = 0.5
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI()

if safemode:
    fn = "rag_safemode_*.pkl"
    prefix = "<safemode>"
else:
    fn = "rag_*.pkl"
    prefix = ""
backup = sorted(glob.glob(fn), key=lambda x:float(re.findall("(\d+)",x)[0]))
if len(backup) >= 1:
    last = int(re.findall("(\d+)", backup[-1])[0])
    records = pickle.load(open(backup[-1], 'rb'))
else:
    last = 0
    records = []

pool = ThreadPool(processes=4)
for i, data in enumerate(ds['train']):
    if i <= last:
        continue
    chunks = get_chunks(data['text'], CHUNK_SIZE)
    print(f"{prefix}i#{i}@chunks#{len(chunks)}")
    if i%100 == 0:
        print(f"{prefix}-----------------------------------")
        print(chunks[0])
        print(f"{prefix}-----------------------------------")

    # max NUM_CHUNK for each document
    indices = list(range(0, len(chunks)))
    random.shuffle(indices)
    thread_result = []
    for k, j in enumerate(indices):
        chunk = chunks[j]
        if k >= NUM_CHUNK:
            break
        # Generate Q
        if USE_TAIDE:
            Q = generate_q_taide(headers, requests, chunk, NUM_Q)
        else:
            Q = generate_q(client, chunk, NUM_Q)
        if j%10 == 0:
            print(Q)

        if safemode:
            random.shuffle(Q)
        for q in Q:
            async_result = pool.apply_async(prepare_sample, (client, headers, requests, chunk, q, chunks, i, j))
            thread_result.append(async_result)
            if safemode:
                break

    for async_result in thread_result:
        sample = async_result.get()
        if sample is not None:
            records.append(sample)

    if i%100 == 0:
        print(f"{prefix}record@{i}#{len(records)}#{records[-1]}")
        if safemode:
            filename = f"rag_safemode_{i}.pkl"
        else:
            filename = f"rag_{i}.pkl"
        with open(filename, 'wb') as outp:
            pickle.dump(records, outp, pickle.HIGHEST_PROTOCOL)

# close the thread pool
pool.close()
# block until all tasks are complete and threads close
pool.join()
# print the records
# print(records)

rag_ft_data = pandas.DataFrame(records)
# convert dataframe to dataset
rag_ft_data = Dataset.from_pandas(rag_ft_data)
rag_ft_data = rag_ft_data.shuffle(seed=42)
rag_ft_data = DatasetDict({'train': rag_ft_data})
print(rag_ft_data)

if safemode:
    config_name = 'v1.0s'
else:
    config_name = 'v1.0'
rag_ft_data.push_to_hub("g-taide/rag_ft_data", config_name=config_name, private=True)
dataset = load_dataset("g-taide/rag_ft_data", config_name)
print(dataset['train'][0]['messages'])
